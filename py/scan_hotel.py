import os
import re
import json
import requests
import concurrent.futures
from urllib.parse import urlparse
from tqdm import tqdm

# ================= é…ç½®åŒº =================
HOTEL_DIR = "./hotel"
MAP_FILE = "py/scan_map.json"
BLACKLIST_FILE = "py/blacklist.json"
TIMEOUT = 3
MAX_WORKERS = 100
HEADERS = {"User-Agent": "Lavf/58.29.100"}
# ==========================================

def load_json(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except Exception:
            return []
    return []

def save_json(file_path, data):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def check_url(url):
    """å•é“¾æ¥æ¢æµ‹"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT, stream=True)
        if r.status_code == 200:
            return url
    except:
        pass
    return None

def run_scan():
    print("\n" + "="*50)
    print("ğŸš€ é…’åº—æºåŸºå› æ‰«æä»»åŠ¡ (é¢„æ£€ä¼˜åŒ–ç‰ˆ)")
    print("="*50)

    blacklist = set(load_json(BLACKLIST_FILE))
    scan_results = [] # æœ€ç»ˆå­˜æ´»æ˜ å°„
    
    # 1. æå–æ‰€æœ‰åŸå§‹åŸºå› 
    raw_genes = [] 
    if not os.path.exists(HOTEL_DIR): return

    for file in os.listdir(HOTEL_DIR):
        if file.endswith(".m3u") and not file.startswith("REBORN"):
            file_path = os.path.join(HOTEL_DIR, file)
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
                urls = re.findall(r'http://([\d\.]+:\d+)(/[^\s,]+)', content)
                for host, path in urls:
                    if host not in blacklist:
                        raw_genes.append({"host": host, "path": path})

    # å»é‡
    unique_genes = {g['host']: g['path'] for g in raw_genes}
    print(f"ğŸ“‹ å‘ç°å¾…æ£€æµ‹åŸå§‹ Host: {len(unique_genes)} ä¸ª")

    # 2. ç¬¬ä¸€é˜¶æ®µï¼šé¢„æ£€åŸå§‹ IP æ˜¯å¦å­˜æ´»
    print(f"ğŸ” æ­£åœ¨è¿›è¡Œç¬¬ä¸€é˜¶æ®µï¼šåŸå§‹ IP è‡ªæ£€...")
    survived_original_hosts = set()
    to_scan_tasks = {} # çœŸæ­£éœ€è¦æ‰«æ®µçš„

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_host = {executor.submit(check_url, f"http://{h}{p}"): h for h, p in unique_genes.items()}
        for future in concurrent.futures.as_completed(future_to_host):
            host = future_to_host[future]
            if future.result():
                survived_original_hosts.add(host)
                # è‡ªå·±å°±æ˜¯æ´»çš„ï¼Œç›´æ¥å­˜å…¥ç»“æœ
                scan_results.append({"old_host": host, "new_host": host, "path": unique_genes[host]})
            else:
                # åŸå§‹ IP æŒ‚äº†ï¼Œå‡†å¤‡æ‰«å®ƒæ‰€åœ¨çš„ C æ®µ
                prefix = ".".join(host.split('.')[:3])
                port = host.split(':')[-1]
                key = f"{prefix}:{port}"
                if key not in to_scan_tasks:
                    to_scan_tasks[key] = {"old_host": host, "path": unique_genes[host]}

    print(f"âœ… è‡ªæ£€å®Œæˆ: {len(survived_original_hosts)} ä¸ªåŸå§‹ IP ä¾ç„¶å­˜æ´»")
    print(f"ğŸ“¡ å‰©ä½™ {len(to_scan_tasks)} ä¸ªç½‘æ®µéœ€è¦æ‰«æ®µå¤æ´»")

    # 3. ç¬¬äºŒé˜¶æ®µï¼šé’ˆå¯¹å¤±è”ç½‘æ®µè¿›è¡Œ C æ®µæ‰«æ
    new_dead_hosts = []
    for key, info in to_scan_tasks.items():
        prefix, port = key.split(':')
        scan_list = [f"http={prefix}.{i}:{port}{info['path']}" for i in range(1, 256)]
        
        valid_found = []
        pbar = tqdm(total=len(scan_list), desc=f"ğŸ“¡ æ‰«ææ®µ {prefix}.x", bar_format='{l_bar}{bar:20}{r_bar}')
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(check_url, u.replace("http=","http://")) for u in scan_list]
            for future in concurrent.futures.as_completed(futures):
                res = future.result()
                if res:
                    new_host = urlparse(res).netloc
                    valid_found.append(new_host)
                    pbar.write(f"  âœ¨ [å¤æ´»] -> {new_host}")
                pbar.update(1)
        pbar.close()

        if not valid_found:
            new_dead_hosts.append(info['old_host'])
        else:
            for v_host in valid_found:
                scan_results.append({"old_host": info['old_host'], "new_host": v_host, "path": info['path']})

    # 4. æŒä¹…åŒ–
    save_json(MAP_FILE, scan_results)
    if new_dead_hosts:
        updated_blacklist = list(set(list(blacklist) + new_dead_hosts))
        save_json(BLACKLIST_FILE, updated_blacklist)

    print(f"\nâœ¨ æ‰«æç»“æŸï¼æ€»è®¡å¯ç”¨ Host: {len(scan_results)} | å½»åº•å¤±æ•ˆæ‹‰é»‘: {len(new_dead_hosts)}")

if __name__ == "__main__":
    run_scan()
