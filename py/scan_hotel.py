import os
import re
import json
import requests
import concurrent.futures
from urllib.parse import urlparse
from tqdm import tqdm

# ================= é…ç½®åŒº =================
HOTEL_DIR = "./hotel"
MAP_FILE = "py/scan_map.json"
BLACKLIST_FILE = "py/blacklist.json"
TIMEOUT = 3
MAX_WORKERS = 100
HEADERS = {"User-Agent": "Lavf/58.29.100"}
# ==========================================

def load_json(file_path):
    """é€šç”¨ JSON åŠ è½½"""
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except Exception as e:
            print(f"âš ï¸ è¯»å– {file_path} å¼‚å¸¸: {e}")
            return []
    return []

def save_json(file_path, data):
    """é€šç”¨ JSON ä¿å­˜"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def run_scan():
    print("\n" + "="*50)
    print("ğŸš€ é…’åº—æºåŸºå› æ‰«æä»»åŠ¡å¼€å§‹ (Actions æŒä¹…åŒ–ç‰ˆ)")
    print("="*50)

    # 1. åŠ è½½é»‘åå• (æ—§çš„ old_host é›†åˆ)
    blacklist = load_json(BLACKLIST_FILE)
    blacklist_set = set(blacklist)
    print(f"ğŸš« å·²åŠ è½½é»‘åå•è®°å½•: {len(blacklist_set)} æ¡")

    tasks = {} 
    if not os.path.exists(HOTEL_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç›®å½• {HOTEL_DIR}")
        return
    
    # 2. æå–åŸºå› ï¼šéå† hotel æ–‡ä»¶å¤¹ä¸‹çš„ m3u
    for file in os.listdir(HOTEL_DIR):
        if file.endswith(".m3u") and not file.startswith("REBORN"):
            file_path = os.path.join(HOTEL_DIR, file)
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    # åŒ¹é… http://ip:port/path æ ¼å¼
                    urls = re.findall(r'http://([\d\.]+:\d+)(/[^\s,]+)', content)
                    for host, path in urls:
                        # å¦‚æœè¯¥æœåŠ¡å™¨åœ¨é»‘åå•ä¸­ï¼Œç›´æ¥è·³è¿‡
                        if host in blacklist_set:
                            continue
                        
                        prefix = ".".join(host.split('.')[:3])
                        port = host.split(':')[-1]
                        key = f"{prefix}:{port}"
                        if key not in tasks:
                            tasks[key] = {"old_host": host, "path": path}
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")

    print(f"ğŸ“Š å¾…æ¢æµ‹æœ‰æ•ˆç½‘æ®µ: {len(tasks)} ä¸ª")
    if not tasks:
        print("ğŸ’¡ æ²¡æœ‰æ–°ä»»åŠ¡ï¼Œè·³è¿‡æ‰«æã€‚")
        return

    scan_results = []
    new_dead_hosts = [] # è®°å½•æœ¬æ¬¡å…¨æ®µå¤±æ•ˆçš„ old_host
    
    # 3. é€æ®µæ¢æµ‹
    for key, info in tasks.items():
        prefix, port = key.split(':')
        # æ„é€ è¯¥ C æ®µæ‰€æœ‰ 255 ä¸ªå¯èƒ½çš„ IP åœ°å€
        scan_list = [f"http://{prefix}.{i}:{port}{info['path']}" for i in range(1, 256)]
        
        valid_found = []
        pbar = tqdm(total=len(scan_list), desc=f"ğŸ“¡ {prefix}.x", bar_format='{l_bar}{bar:20}{r_bar}')
        
        def check_url(url):
            try:
                # ä½¿ç”¨ stream=True é¿å…ä¸‹è½½å¤§æ–‡ä»¶ï¼Œåªæ£€æµ‹å“åº”å¤´
                r = requests.get(url, headers=HEADERS, timeout=TIMEOUT, stream=True)
                if r.status_code == 200:
                    return url
            except:
                pass
            return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(check_url, u) for u in scan_list]
            for future in concurrent.futures.as_completed(futures):
                res = future.result()
                if res:
                    new_host = urlparse(res).netloc
                    valid_found.append(new_host)
                    pbar.write(f"  âœ¨ [å­˜æ´»] -> {new_host}")
                pbar.update(1)
        pbar.close()
        
        # åˆ¤å®šï¼šå¦‚æœè¿™ä¸€æ®µ 1-255 ä¸€ä¸ªæ´»çš„éƒ½æ²¡æœ‰ï¼ŒæŠŠåŸå§‹ old_host æ‹‰é»‘
        if not valid_found:
            new_dead_hosts.append(info['old_host'])
            print(f"ğŸ’€ æ®µ {prefix}.x ç¡®è®¤å¤±æ•ˆï¼ŒåŠ å…¥é»‘åå•")
        else:
            for v_host in valid_found:
                scan_results.append({
                    "old_host": info['old_host'], 
                    "new_host": v_host,
                    "path": info['path']
                })

    # 4. ç»“æœæŒä¹…åŒ–
    # ä¿å­˜å­˜æ´» IP æ˜ å°„ä¾› rebuild_m3u.py ä½¿ç”¨
    if scan_results:
        save_json(MAP_FILE, scan_results)
        print(f"ğŸ’¾ å­˜æ´»æ˜ å°„å·²æ›´æ–°: {MAP_FILE}")

    # æ›´æ–°å¹¶ä¿å­˜é»‘åå•
    if new_dead_hosts:
        updated_blacklist = list(set(blacklist + new_dead_hosts))
        save_json(BLACKLIST_FILE, updated_blacklist)
        print(f"ğŸš« é»‘åå•å·²åŒæ­¥ï¼Œå½“å‰æ€»æ•°: {len(updated_blacklist)}")

    print("\n" + "="*50)
    print(f"âœ… ä»»åŠ¡ç»“æŸã€‚æ–°å¢å­˜æ´»: {len(scan_results)} | æ–°æ‹‰é»‘: {len(new_dead_hosts)}")
    print("="*50 + "\n")

if __name__ == "__main__":
    run_scan()
